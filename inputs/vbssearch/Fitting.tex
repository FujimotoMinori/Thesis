\chapter{Statistical treatment}
\label{chap:statistics}

\section{Fiducial cross section extraction}
In addition to the observation of the EW VV+jj production, the fiducial cross section measurement is performed at the same time.
The fiducial cross section, which is the cross section that takes the detector acceptance into account is exploited and it allows easier comparisons with other theoretical predictions.
The fiducial selection is done at MC samples using the stable final state particle, and these cuts are chosen to be as similar as possible to those at reconstruction level selection, described in Chapter~\ref{chap:eventselection}.
These selections are summarized in the Table~\ref{tbl:vbs_fid_sel}. 

\begin{table}[h]
\begin{center}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{|l|c|c|c|} \hline
%
\multicolumn{4}{|c|}{Object selection} \\ \hline
%
Leptons      & \multicolumn{3}{c|}{$\pt > 27$~GeV, $|\eta| < 2.5$ } \\
%
Small-R jets & \multicolumn{3}{c|}{$\pt > 20$~GeV if $|\eta| < 2.5$ and $\pt > 30$~GeV if $2.5 < |\eta| < 4.5$} \\
%
Large-R jets & \multicolumn{3}{c|}{$\pt > 200$~GeV, $|\eta| < 2.0$ } \\
%
\hline
\multicolumn{4}{|c|}{Event selection} \\
\hline
%
\multirow{3}{*}{Leptonic $V$ selection}
  &  0-lep & 1-lep & 2-lep \\
  &  $\met > 200$~GeV & One lepton       & Two leptons \\
  &                   & $\met > 80$~GeV  &             \\
\cline{2-4}
%
\multirow{2}{*}{Tagging jets}
  & \multicolumn{3}{c|}{$\eta_{\mathrm{tag}\ j_1} \cdot \eta_{\mathrm{tag}\ j_2} < 0$, highest $M_{jj}$} \\
  & \multicolumn{3}{c|}{$p^{tag\ jet_{1,2}}_{T} > 30$~\textrm{GeV}, $M_{jj}$>400~\textrm{GeV}}  \\
  & \multicolumn{3}{c|}{b-hadron associated jets veto} \\
\cline{2-4}
%
\multirow{5}{*}{Hadronic $V$ selection}
  & Merged  & \multicolumn{2}{c|}{Leading \pt large-R jet} \\
  & & \multicolumn{2}{c|}{$64 < M_{J} < 106$~\textrm{GeV} } \\
\cline{2-4}
%
  & \multirow{3}{*}{Resolved}
    & \multicolumn{2}{c|}{Two leading \pt small-R jets} \\
  & & \multicolumn{2}{c|}{$p^{j_{1}}_{T}>$40~GeV, $p^{j_{2}}_{T}>$20~GeV } \\
  & & \multicolumn{2}{c|}{$64 < M_{jj} < 106$~\textrm{GeV} } \\
\cline{2-4}
%
\multirow{1}{*}{Extra b-jets Veto selection}
  & \multicolumn{3}{c|}{ $N_{ExtraBJets} = 0$ (only for 1-lepton channel) } \\
\cline{2-4}
%
\multirow{1}{*}{Additional (EW)Top Veto selection}
  & \multicolumn{3}{c|}{ $M_{jjj} > 220~\mathrm{GeV}$ (only resolved) } \\
%
\hline

\end{tabular}
}
\caption{Summary of the event selection performed at the particle level to define the fiducial regions.}
\label{tbl:vbs_fid_sel}
\end{center}
\end{table}

The expected cross-section from the standard model prediction, $\sigma_{EW VVjj \mathrm{, \ SM}}$ can be exploited by using the the truth event yields, yields after applying the fiducial selections, and the luminosity.
Fiducial cross-section is evaluated by scailing the measured signal strength $\mu$, defined as;
\begin{equation}
\mu = \frac{\sigma_{EW VVjj \mathrm{, \ obs}}}{\sigma_{EW VVjj \mathrm{, \ SM}}}
\end{equation}
where $\sigma_{EW VVjj \mathrm{, \ obs}}$ is the cross-section of the EW VV+jj process observed with data, the exact fiducial cross section we finally want to measure.
The $\mu$ will be obtained in section~\ref{sec:likelihood}.

\section{Fitting setups}
A simultaneous fit to all the merged and resolved signal and control regions is performed.
The final discriminant in each region is summarized in table.
\begin{table}[htbp]
 \footnotesize
\begin{center}
\begin{tabular}{ | c | c | c | c |} \hline
                      & 0-lepton        & 1-lepton       & 2-lepton  \\ \hline \hline
Merged HP SR          &  RNN score      &  RNN score     & RNN score     \\ \hline
Merged LP SR          &  RNN score      &  RNN score     & RNN score     \\ \hline
Resolved SR           &  RNN score      &  RNN score     & RNN score     \\ \hline \hline
V+jet Merged CR       & $m^{tag}_{jj}$  & $m^{tag}_{jj}$ & $m^{tag}_{jj}$\\ \hline 
V+jet Resolved CR     & $m^{tag}_{jj}$  & $m^{tag}_{jj}$ & $m^{tag}_{jj}$\\ \hline
Top Merged CR         &                 & yields         &               \\ \hline
Top Resolved CR       &                 & yields         &               \\ \hline
\end{tabular}
\caption{\label{tab:discriminant} Final discriminant in each region used in the fitting. }
\end{center}
\end{table}
In the signal regions the RNN score is used, while in the CRs the $m^{tag}_{jj}$ is employed in order to give a better constrains to the $m^{tag}_{jj}$ reweighting uncertainty. In the Top CR single bin setting is adapted since this CR is for constraining $t\bar{t}$ background normalization and there is no need to use the shape. 
The background is estimated using the MC for the RNN shape in SRs, while the normalization is estimated using the data in CRs.
The normalization in SR is extrapolated from the CR. 
The ratio of the normalization in SR and in CR rely on the nominal MC modeling, while the theoretical variation is taken into account as systematic as described in section~\ref{sec:TheoryUnc}.
The another NP which changes the ratio of the SR and CR is the uncertainties on boson tagging scale factor, as described in section~\ref{sec:ExperimUnc}.
All systematic uncertainties are included in the fit as nuisance parameter. Since the definition of the CR is basically same but inverted the mass window or the boson tagging, it is designed that most of the NPs are expected to be canceled each other out.

\section{Likelihood function definition}
\label{sec:likelihood}
Binned maximum likelihood fit is performed to get the signal strength $\mu$. The likelihood fuction is written as:
\begin{equation}
\label{eq:poisson}
\mathcal{L}(\mu, \theta) = \operatorname{P}\left(N_{i} \mid \mu s_{i}+b_{i}\right) \cdot G(\theta) 
\end{equation}
where the P is the Poisson probability terms:
\begin{equation}
\operatorname{P}\left(N_{i} \mid \mu s_{i}+b_{i}\right) = \prod_{i \in \text { bins }} \frac{\left(\mu s_{i}+b_{i}\right)^{N_{i}}}{N_{i} !} e^{-\left(\mu s_{i}+b_{i}\right)} 
\end{equation}
%\begin{equation}
%\mathcal{L}_{\text {Meas }}(\mu, \theta)=\prod_{i \in \text { bins }} \operatorname{Pois}\left(N_{i} \mid \mu s_{i}+b_{i}\right)=\prod_{i \in \text { bins }} \frac{\left(\mu s_{i}+b_{i}\right)^{N_{i}}}{N_{i} !} e^{-\left(\mu s_{i}+b_{i}\right)}
%\end{equation}
$s_i$ and $b_i$ is the expected number of the signal and the background events yield in bin $i$, and $N_i$ is the numbers of the observed events from data in the bin. 
The $\mu$ is also called as the parameter of interest (POI). $\theta$ is a set of nuisance parameters (NPs) associated with each systematic uncertainty. $\theta$ affects the signal and background yields so it can be written like $s_{i}=s_{i}(\theta)$ and $b_{i}=b_{i}(\theta)$. 
The second term in equation~\ref{eq:poisson} is often called penalty term, to represent the supplementary information of the systematic uncertainty effect. It is a product from all single uncertainty $\theta_j$; $G(\theta) = \prod_{j}G_{j}(\theta_{j}$). 
In this analysis, this term is given by the Gaussian distribution with a standard deviation of $\sigma = 1$. It is designed so that $\theta_j = 0$ is at its nominal value, and $\theta_j = \pm 1$ corresponds to the $\pm 1 \sigma$ variations of the systematic source. 
Some parameters for example Z+jets background normalization factors are called floating parameters, which is determined completely from the data and do not have this penalty terms.

The statistical uncertainties of the total background is also included to the likelihood function as $\gamma$ parameter. 
The additinal nuisance parameter is added to the initial $\mathcal{L}$ as $\gamma_i$ for each bin, where $\gamma_i$ is the total statistical uncertainty of a specific histogram bin i. 
The poisson distribution is expected for the gamma parameters, conventionally they are paremetrized so that the nominal expectation is 1 and its variation corresponds to $\pm 1 \sigma$, where the $\sigma$ is the MC statistical uncertainty of the bin i.
%\begin{equation}
%{Pois}\left(N_{i} \mid \mu s_{i}+\gamma_{i}b_{i}\right)
%\end{equation}
%With this modification the background estimation in each bin about the nominal value of $\gamma = 1$. 
%Another likelihood term is added to represent the statistical uncertainty:
%\begin{equation}
%\mathcal{L}_{\mathrm{BkgStat}}\left(\gamma_{i}\right)=\prod_{i \in \mathrm{bins}} \operatorname{Pois}\left(n_{i} \mid \gamma_{i} n_{i}\right)
%\end{equation}
% ??? don't understand explanations here
%The full likelihood can be described as:
%\begin{equation}
%\mathcal{L}(\mu, \theta)=\mathcal{L}_{\text {Meas }}\left(\mu, \theta, \gamma_{i}\right) \cdot \mathcal{L}_{\text {Prior }}(\theta) \cdot \mathcal{L}_{\text {BkgStat }}\left(\gamma_{i}\right)
%\end{equation}
An estimate on $\mu$ is obtained by maximizing the likelihood function~\ref{eq:poisson} with respect to all the parameters.  

The profile likelihood is given to test a hypothesized value of $\mu$:
\begin{equation}
\lambda(\mu) = \frac{\mathcal{L}\left(\mu, \hat{\hat{\theta}}_{\mu}\right)}{\mathcal{L}(\hat{\mu}, \hat{\theta})}
\end{equation}
where $\hat{\mu}$ and $\hat{\theta}$ are the parameters that maximize $\mathcal{L}$, and $\hat{\hat{\theta}}$ is the best fit value that masimize $\mathcal{L}$ for a certain signal strength $\mu$.
A test statistic is constructed with:
\begin{equation}
q_{\mu}= -2 \log \lamda (\mu)
\end{equation}
The higher $q_{\mu}$ values show the incompatibility of a specific hypothesis $\mu$ and the observed data.
% need this?
The level of the incompatibility is quantified by the p-value:
\begin{equation}
p_{\mu}=\int_{q_{\mu, \mathrm{obs}}}^{\infty} f\left(q_{\mu} \mid \mu\right) d q_{\mu}
\end{equation}
where the $q_{\mu, \mathrm{obs}}$ is the value of the test statistic $q_{\mu}$ observed from the data, and $f\left(q_{\mu} \mid \mu\right)$ denotes the probability density function (pdf) of $q_{\mu}$ under the assumption of the signal strength $\mu$.
%
The significance Z is estimated by calculating the $q_{\mu}$ for the background only hypothesis,


%p-value and one-sided CL upper limit
%The $p_0$ is case is used for rejecting the background only hypothesis ($\mu = 0$).
The p-value is often converted into an equivalent significance, Z:
\begin{equation}
Z=\Phi^{-1}(1-p)
\end{equation}
where $\Phi^{-1}$ is the inverse of the cumulative standard Gaussian.
Typically significance of Z = 5~$\sigma$ is used as an appropriate level to constitute a discovery ($p_0 = 2.87 \times 10^{-7})$). For excluding a signal hypothesis, threshold of the p-value is set at 0.05 (i.e., 95$\%$ confidence level, Z = 1.64~$\sigma$).
To obtain the p-value, $f\left(q_{\mu} \mid \mu \right)$ is required and this can be derived by sampling the distributions with the Monte Carlo method, which is a high computationally complex job. Instead, non-central $\chi^2$ distribution can be used.
%pu here reference
%for aQGC limit - CI or CL limit

\section{Smoothing}
The smoothing algorithm is applied to alleviate statistical fluctuations, that might create suspicious effects in the fit.
During this procedure bins are migrated from left to right until the statistical uncertainty per bin to be less than 5$\%$. The nominal and smoothed variation histograms are then compared to derive the up and down uncertainties. The resulting uncertainties are associated to the initial finer binned distribution. 

\section{Pruning}
Systematic variations that have a very small effect and are negligible for the measurement are pruned away. The uncertainties removed are:
  \begin{itemize}
   \item  Normalization uncertainties with a less than 5$\%$ relative variation effects or same sign effects (relative variation being positive (or negative) for both the up and down uncertainty)
   \item  Shape uncertainties with less than 0.5$\%$ effect for all bins of the distribution or missing one of the up or down variations.
    \end{itemize}
    
The nominal fit result in terms of $\mu$ and $\sigma_{\mu}$ is obtained by maximizing the likelihood function with respect to all parameters which was not pruned away.
%This is referred to as the maximized log-likelihood value, MLL.
%The test statistic $q_\mu$ is then constructed according to the profile likelihood: $q_\mu = 2 \ln (\mathcal{L} (\mu, \hat{\hat{\theta_\mu}})/\mathcal{L} (\hat{\mu}, \hat{\theta}))$, where $\hat{\mu}$ and $\hat{\theta}$ are the parameters that maximize the likelihood (with the constraint $0 \leq \hat{\mu} \leq \mu$), and $\hat{\hat{\theta}}_\mu$ are the nuisance parameter values that maximize the likelihood for a given $\mu$.
%This test statistic is used to measure the compatibility of the background-only model with the observed data and for exclusion intervals derived with the $CL_s$ method~\cite{Cowan:2010js}.
%The limit set on $\mu$ is then translated into a limit on the signal cross section times branching ratio, using the theoretical cross section and branching ratio for the given signal model.


