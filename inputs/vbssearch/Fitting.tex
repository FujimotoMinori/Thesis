\chapter{Statistical treatment}
\label{chap:statistics}

\section{Fiducial cross-section extraction}
\label{sec:crosssection}
In addition to the observation of the EW VV+jj production, the fiducial cross-section measurement is performed at the same time.
The fiducial cross-section, which is the cross-section that takes the detector acceptance into account is exploited and it allows easier comparisons with other theoretical predictions.
The fiducial selection is done at MC samples using the stable final state particle, and these cuts are chosen to be as similar as possible to those at reconstruction level selection, described in Chapter~\ref{chap:eventselection}.
These selections are summarized in the table~\ref{tbl:vbs_fid_sel}. 

\begin{table}[h]
\begin{center}
\resizebox{0.8\textwidth}{!}{
\begin{tabular}{|l|c|c|c|} \hline
%
\multicolumn{4}{|c|}{Object selection} \\ \hline
%
Leptons      & \multicolumn{3}{c|}{$\pt > 27$~GeV, $|\eta| < 2.5$ } \\
%
Small-R jets & \multicolumn{3}{c|}{$\pt > 20$~GeV if $|\eta| < 2.5$ and $\pt > 30$~GeV if $2.5 < |\eta| < 4.5$} \\
%
Large-R jets & \multicolumn{3}{c|}{$\pt > 200$~GeV, $|\eta| < 2.0$ } \\
%
\hline
\multicolumn{4}{|c|}{Event selection} \\
\hline
%
\multirow{3}{*}{Leptonic $V$ selection}
  &  0-lep & 1-lep & 2-lep \\
  &  $\met > 200$~GeV & One lepton       & Two leptons \\
  &                   & $\met > 80$~GeV  &             \\
\cline{2-4}
%
\multirow{2}{*}{Tagging jets}
  & \multicolumn{3}{c|}{$\eta_{\mathrm{tag}\ j_1} \cdot \eta_{\mathrm{tag}\ j_2} < 0$, highest $M_{jj}$} \\
  & \multicolumn{3}{c|}{$p^{tag\ jet_{1,2}}_{T} > 30$~\textrm{GeV}, $M_{jj}$>400~\textrm{GeV}}  \\
  %& \multicolumn{3}{c|}{b-hadron associated jets veto} \\
\cline{2-4}
%
\multirow{5}{*}{Hadronic $V$ selection}
  & Merged  & \multicolumn{2}{c|}{Leading \pt large-R jet} \\
  & & \multicolumn{2}{c|}{$64 < M_{J} < 106$~\textrm{GeV} } \\
\cline{2-4}
%
  & \multirow{3}{*}{Resolved}
    & \multicolumn{2}{c|}{Two leading \pt small-R jets} \\
  & & \multicolumn{2}{c|}{$p^{j_{1}}_{T}>$40~GeV, $p^{j_{2}}_{T}>$20~GeV } \\
  & & \multicolumn{2}{c|}{$64 < M_{jj} < 106$~\textrm{GeV} } \\
\cline{2-4}
%
\multirow{1}{*}{Extra b-jets Veto selection}
  & \multicolumn{3}{c|}{ $N_{ExtraBJets} = 0$ (only for 1-lepton channel) } \\
\cline{2-4}
%
\multirow{1}{*}{Additional (EW)Top Veto selection}
  & \multicolumn{3}{c|}{ $M_{jjj} > 220~\mathrm{GeV}$ (only resolved) } \\
%
\hline

\end{tabular}
}
\caption{Summary of the event selection performed at the particle level to define the fiducial regions.}
\label{tbl:vbs_fid_sel}
\end{center}
\end{table}

The expected cross-section from the standard model prediction, $\sigma_{EW VVjj \mathrm{, \ SM}}$ can be exploited by using the truth event yields, which are the yields after applying the fiducial selections, and the luminosity.
Fiducial cross-section is evaluated by scaling the measured signal strength $\mu$, defined as;
\begin{equation}
\mu = \frac{\sigma_{EW VVjj \mathrm{, \ obs}}}{\sigma_{EW VVjj \mathrm{, \ SM}}}
\end{equation}
where $\sigma_{EW VVjj \mathrm{, \ obs}}$ is the cross-section of the EW VV+jj process observed with data, the exact fiducial cross-section we finally want to measure.
How to obtain $\mu$ is described in section~\ref{sec:likelihood}.
%The $\mu$ will be obtained in section~\ref{sec:likelihood}.

\section{Likelihood function definition}
\label{sec:likelihood}
This section describes how to extract the signal strength $\mu$ and the significance.
Binned maximum likelihood fit is performed to get the signal strength $\mu$. 
%Following description is adapted from Ref.~\cite{} and the implementation is done with RooStat tools~\cite{}.
The likelihood function is written as:
\begin{equation}
\label{eq:poisson}
\mathcal{L}(\mu, \theta) = \operatorname{P}\left(N_{i} \mid \mu s_{i}+b_{i}\right) \cdot G(\theta) 
\end{equation}
where the P is the Poisson probability terms:
\begin{equation}
\operatorname{P}\left(N_{i} \mid \mu s_{i}+b_{i}\right) = \prod_{i \in \text { bins }} \frac{\left(\mu s_{i}+b_{i}\right)^{N_{i}}}{N_{i} !} e^{-\left(\mu s_{i}+b_{i}\right)} 
\end{equation}
%\begin{equation}
%\mathcal{L}_{\text {Meas }}(\mu, \theta)=\prod_{i \in \text { bins }} \operatorname{Pois}\left(N_{i} \mid \mu s_{i}+b_{i}\right)=\prod_{i \in \text { bins }} \frac{\left(\mu s_{i}+b_{i}\right)^{N_{i}}}{N_{i} !} e^{-\left(\mu s_{i}+b_{i}\right)}
%\end{equation}
$s_i$ and $b_i$ are the expected number of the signal and the background events yield in bin $i$, and $N_i$ is the number of the observed events from data in the bin. 
The $\mu$ is also called as the parameter of interest (POI). $\theta$ is a set of nuisance parameters (NPs) associated with each systematic uncertainty. $\theta$ affects the signal and background yields so it can be written like $s_{i}=s_{i}(\theta)$ and $b_{i}=b_{i}(\theta)$. 
The second term in equation~\ref{eq:poisson}, $G(\theta)$ is often called penalty term, to represent the supplementary information of the systematic uncertainty effect, and to constrain each uncertainty. It is a product from all single uncertainty $\theta_j$; $G(\theta) = \prod_{j}G_{j}(\theta_{j}$). 
In this analysis, this term is given by the Gaussian distribution with a standard deviation of $\sigma = 1$. It is designed so that $\theta_j = 0$ corresponds to the nominal value, and $\theta_j = \pm 1$ corresponds to the $\pm 1 \sigma$ variations of the systematic source. 
Some parameters for example Z+jets background normalization factors do not have these penalty terms and are determined completely from the data, called floating parameters.
The statistical uncertainties of the total background are also included in the likelihood function as $\gamma$ parameter. 
The additional nuisance parameter called gamma parameter is also added to the initial $\mathcal{L}$ as $\gamma_i$ for each bin, where $\gamma_i$ is the total statistical uncertainty of a specific histogram bin i. 
The Poisson distribution is expected for the gamma parameters, conventionally they are parametrized so that the nominal expectation is 1 and its variation corresponds to $\pm 1 \sigma$, where the $\sigma$ is the MC statistical uncertainty of the bin i.
%\begin{equation}
%{Pois}\left(N_{i} \mid \mu s_{i}+\gamma_{i}b_{i}\right)
%\end{equation}
%With this modification the background estimation in each bin about the nominal value of $\gamma = 1$. 
%Another likelihood term is added to represent the statistical uncertainty:
%\begin{equation}
%\mathcal{L}_{\mathrm{BkgStat}}\left(\gamma_{i}\right)=\prod_{i \in \mathrm{bins}} \operatorname{Pois}\left(n_{i} \mid \gamma_{i} n_{i}\right)
%\end{equation}
% ??? don't understand explanations here
%The full likelihood can be described as:
%\begin{equation}
%\mathcal{L}(\mu, \theta)=\mathcal{L}_{\text {Meas }}\left(\mu, \theta, \gamma_{i}\right) \cdot \mathcal{L}_{\text {Prior }}(\theta) \cdot \mathcal{L}_{\text {BkgStat }}\left(\gamma_{i}\right)
%\end{equation}
An estimate on $\mu$ is obtained by maximizing the likelihood function~\ref{eq:poisson} with respect to all the parameters.  

In order to test a hypothesized value of $\mu$, the profile likelihood is given:
\begin{equation}
\lambda(\mu) = \frac{\mathcal{L}\left(\mu, \hat{\hat{\theta}}\right)}{\mathcal{L}(\hat{\mu}, \hat{\theta})}
\end{equation}
where $\hat{\mu}$ and $\hat{\theta}$ are the parameters that maximize $\mathcal{L}$, and $\hat{\hat{\theta}}$ is the best fit value that maximize $\mathcal{L}$ for a certain signal strength $\mu$.

A test statistic to test $\mu$ is constructed with:
\begin{equation}
t_{\mu}= -2 \log \lambda (\mu)
\end{equation}
The higher $t_{\mu}$ values correspond to increasing incompatibility between a specific hypothesis $\mu$ and the observed data.
The level of the incompatibility is quantified by the p-value:
\begin{equation}
p_{\mu}=\int_{t_{\mu, \mathrm{obs}}}^{\infty} f\left(t_{\mu} \mid \mu\right) d t_{\mu}
\end{equation}
where the $t_{\mu, \mathrm{obs}}$ is the value of the test statistic $t_{\mu}$ observed from the data, and $f\left(t_{\mu} \mid \mu\right)$ denotes the probability density function (pdf) of $t_{\mu}$ under the assumption of the signal strength $\mu$.
%
The p-value is often converted into an equivalent significance, Z:
\begin{equation}
Z=\Phi^{-1}(1-p)
\end{equation}
where $\Phi^{-1}$ is the inverse of the cumulative standard Gaussian.
A given hypothesis is excluded if the p-value is sufficiently small, that is, Z is sufficiently large.

To reject the background-only hypothesis and claim the discovery of a new phenomenon, the significance is estimated for the background-only hypothesis, corresponding to $\mu = 0$.
This background-only hypothesis in the case of $p_0$ is regarded as a null hypothesis. 
This is to be tested against the alternative hypothesis, which includes both background and the EW VV+jj signal.
Typically significance of Z = 5~$\sigma$ is used as an appropriate level to constitute a discovery ($p_0 = 2.87 \times 10^{-7}$). 

%for aQGC limit - CI 
%On the other hand, for pupose of excluding a signal hypothesis, 
%p-value and one-sided CL upper limit
%The $f\left(t_{\mu} \mid \mu\right)$ 

%On the other hand, for purpose of excluding a signal hypothesis, threshold of the p-value is often set at 0.05 (i.e., 95$\%$ confidence level, Z = 1.64~$\sigma$).
%To obtain the p-value, $f\left(q_{\mu} \mid \mu \right)$ is required and this can be derived by sampling the distributions with the Monte Carlo method, which is a high computationally complex job. Instead, non-central $\chi^2$ distribution can be used.
%pu here reference
%for aQGC limit - CI or CL limit

\section{Fitting setups}
A simultaneous likelihood fit to all 18 merged and resolved signal and control regions is performed.
The final discriminant in each region is summarized in table~\ref{tab:discriminant}.
\begin{table}[htbp]
 \footnotesize
\begin{center}
\begin{tabular}{ | c | c | c | c |} \hline
                      & 0-lepton        & 1-lepton       & 2-lepton  \\ \hline \hline
Merged HP SR          &  RNN score      &  RNN score     & RNN score     \\ \hline
Merged LP SR          &  RNN score      &  RNN score     & RNN score     \\ \hline
Resolved SR           &  RNN score      &  RNN score     & RNN score     \\ \hline \hline
V+jet Merged CR       & $m^{tag}_{jj}$  & $m^{tag}_{jj}$ & $m^{tag}_{jj}$\\ \hline 
V+jet Resolved CR     & $m^{tag}_{jj}$  & $m^{tag}_{jj}$ & $m^{tag}_{jj}$\\ \hline
Top Merged HP CR      &                 & yields         &               \\ \hline
Top Merged LP CR      &                 & yields         &               \\ \hline
Top Resolved CR       &                 & $m^{tag}_{jj}$ &               \\ \hline
\end{tabular}
\caption{\label{tab:discriminant} Final discriminant in each region used in the fitting. }
\end{center}
\end{table}
In the signal regions, the RNN score is used to get maximized sensitivity, while in the CRs the $m^{tag}_{jj}$ is employed in order to give better constraints to the $m^{tag}_{jj}$ reweighting uncertainty. 
In the Top merged CRs single bin setting is adapted since these CRs are for constraining $t\bar{t}$ background normalization and there is no need to use the shape. 
%The pre-fit plots of all regions are shown in figure~\ref{fig:preCR} as the inputs of the final fitting.

The background is estimated using the MC for the RNN shape in SRs, while the normalization is estimated using the data in CRs.
The normalization in the SR is designed to be extrapolated from the CR with the ratio of SR/CR obtained by the nominal MC modeling.
As the NPs which change the ratio of the normalization of SR/CR, theoretical variation and the boson tagging scale factor uncertainty are taken into account.
All systematic uncertainties are included in the fit as NPs with penalty terms or floating parameters as described in section~\ref{sec:likelihood}. Since the definition of the CR is basically same as SR but inverted the mass window or the boson tagging, it is designed that most of the NPs are expected to be canceled each other out.

The normalization of the dominant background (Z+jets, W+jets, $t\bar{t}$) is assigned as the floating parameter, while the background with a small contribution which has a small impact on the signal extraction is assigned as the NP with a penalty term of a certain amount of the error.
The normalization of the floated backgrounds is designed to be obtained separately in the merged region and the resolved region, in order to absorb the effect of the possible mismodeling in the high center-of-mass energy region and/or in merged regions.
%why QCD diboson normalization had the priors
The diboson contribution has anti-correlation with the EW signal. 
Normally it should be constrained with data, but it is assigned with a penalty term currently, due to the lack of a dedicated control region.
The treatment of each background normalization is summarized in table~\ref{tab:bkgnorm}.
The amount of the error of the penalty term is estimated by comparing the nominal MC yields to that of the alternative sample in each analysis region.
An extrapolation uncertainty in the combined fit is also added to take into account differences in the background composition coming from the different lepton channels. The error is obtained from the double ratio between the channel with the largest yield and another channel.
\begin{table}[htbp]
 \footnotesize
\begin{center}
\begin{tabular}{ | c | c | c | c |} \hline
Normalization name        & treatment   & \multicolumn{2}{|c|}{error of the penalty term} \\ \hline \hline
\multicolumn{2}{|c|}{}                     & Merged    & Resolved  \\ \hline 
Z norm.             & Float       &      &\\ \hline
W norm.             & Float       &       &\\ \hline 
$t\bar{t}$ norm.    & Float       &       &\\ \hline
Diboson norm.       &             & 0.5    &  0.3 \\ \hline
Single top norm     &             & \multicolumn{2}{|c|}{0.3}\\ \hline \hline
Z norm 2lep to 0lep       &             & 0.234  &   0.293 \\ \hline
Z norm 2lep to 1lep       &             & 0.454  &  0.410  \\ \hline
W norm 1lep to 0lep       &             & 0.127     & 0.094\\ \hline
$t\bar{t}$ norm 1lep to 0lep     &             & 0.304     & 0.601\\ \hline
Diboson norm 1lep to 0lep    &             & 0.061     &0.078 \\ \hline
Diboson norm 1lep to 2lep       &             & 0.152     & 0.098\\ \hline
\end{tabular}
\caption{\label{tab:bkgnorm} The treatment of the background normalization. The main background sources (Z+jets, W+jets, $t\bar{t}$) are floated in the fitting setup, while the other backgrounds are treated as NPs with penalty terms. The amounts of errors of the penalty terms are shown in the table.  }
\end{center}
\end{table}

\section{Smoothing and pruning}
The smoothing algorithm is applied to the variation of the systematic uncertainties enter into the likelihood fit to alleviate statistical fluctuations, that might create suspicious effects in the fit.
During this procedure, bins are migrated from left to right until the statistical uncertainty per bin is less than 5$\%$. The nominal and smoothed variation histograms are then compared to derive the up and down uncertainties. The resulting uncertainties are associated to the initial finer binned distribution. 

Systematic variations that have a very small effect and are negligible for the measurement are neglected for speeding up the fitting, this procedure is called pruning. 
The uncertainties removed are:
  \begin{itemize}
   \item  Normalization uncertainties with a less than 1~$\%$ relative variation effects or same sign effects (relative variation being positive (or negative) for both the up and down uncertainty)
   \item  Shape uncertainties with less than 1~$\%$ effect for all bins of the distribution or missing one of the up or down variations.
    \end{itemize}
%The nominal fit result in terms of $\mu$ and $\sigma_{\mu}$ is obtained by maximizing the likelihood function with respect to all parameters which was not pruned away.
%This is referred to as the maximized log-likelihood value, MLL.
%The test statistic $q_\mu$ is then constructed according to the profile likelihood: $q_\mu = 2 \ln (\mathcal{L} (\mu, \hat{\hat{\theta_\mu}})/\mathcal{L} (\hat{\mu}, \hat{\theta}))$, where $\hat{\mu}$ and $\hat{\theta}$ are the parameters that maximize the likelihood (with the constraint $0 \leq \hat{\mu} \leq \mu$), and $\hat{\hat{\theta}}_\mu$ are the nuisance parameter values that maximize the likelihood for a given $\mu$.
%This test statistic is used to measure the compatibility of the background-only model with the observed data and for exclusion intervals derived with the $CL_s$ method~\cite{Cowan:2010js}.
%The limit set on $\mu$ is then translated into a limit on the signal cross section times branching ratio, using the theoretical cross section and branching ratio for the given signal model.


